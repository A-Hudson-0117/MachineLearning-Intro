{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected NN (Question 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = '../dataset/cifar10'\n",
    "cifar10     = datasets.CIFAR10(data_path, train=True , download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_map = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n",
    "sample_image = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get std deviation and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = tensor([0.4914, 0.4822, 0.4465])\n",
      "std = tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "data_mean = imgs.view(3, -1).mean(dim=1)\n",
    "data_std  = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "print(f'mean = {data_mean}')\n",
    "print(f'std = {data_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted\n"
     ]
    }
   ],
   "source": [
    "# to save memory\n",
    "del cifar10, cifar10_val, tensor_cifar10, imgs\n",
    "try:\n",
    "    cifar10[0]\n",
    "    print(\"not deleted\")\n",
    "except:\n",
    "    print(\"deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real import  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(data_mean, # mean of entire dataset\n",
    "                             data_std) # std deviation of entire dataset\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(data_mean, data_std)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[sample_image]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "print(img.shape)\n",
    "torch.numel(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-layered NN\n",
    "part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (input_layer): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "  (hidden_activation_1): Tanh()\n",
       "  (hidden_layer_1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (hidden_layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_in = torch.numel(img)  # Number of elements in 3 x 32 x 32 image\n",
    "                         # IE.  3072\n",
    "n_out = len(cifar10_map) # number of possible outputs\n",
    "                         # IE.  10\n",
    "\n",
    "n_inner_1 = n_in // 3\n",
    "n_inner_2 = n_inner_1 // 2\n",
    "n_inner_3 = n_inner_2 // 2\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "        (\n",
    "        \"input_layer\",\n",
    "        nn.Linear(n_in, n_inner_1)\n",
    "        ),\n",
    "        (\n",
    "        \"hidden_activation_1\",\n",
    "        nn.Tanh()\n",
    "        ),\n",
    "        (\n",
    "        \"hidden_layer_1\",\n",
    "        nn.Linear(n_inner_1, n_inner_2)\n",
    "        ),\n",
    "        (\n",
    "        \"hidden_activation_1\",\n",
    "        nn.Tanh()\n",
    "        ),\n",
    "        (\n",
    "        \"hidden_layer_2\",\n",
    "        nn.Linear(n_inner_2, n_inner_3)\n",
    "        ),\n",
    "        (\n",
    "        \"hidden_activation_1\",\n",
    "        nn.Tanh()\n",
    "        ),\n",
    "        (\n",
    "        \"output\",\n",
    "        nn.Linear(n_inner_3, n_out)\n",
    "        )\n",
    "    ]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2498, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[sample_image]\n",
    "\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def training(training_imgs, loss_fn, optimizer, n_epochs:int, report_period:int = 10):\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for img, label in training_imgs:\n",
    "            out = model(img.view(img.shape[0], -1))\n",
    "            loss = loss_fn(out, label)\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        if ((epoch % report_period) == 0) or (epoch == n_epochs):\n",
    "            print(f\"Epoch: {epoch}, Loss: {float(loss):.5f}, Time: {end-start:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (input_layer): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "  (hidden_activation_1): Tanh()\n",
       "  (hidden_layer_1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (hidden_layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    cifar10, \n",
    "                    batch_size=64,\n",
    "                    shuffle=True\n",
    "                    )\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 300\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: nan, Time: 32.12 s\n"
     ]
    }
   ],
   "source": [
    "training(train_loader, \n",
    "         loss, \n",
    "         optimizer, \n",
    "         n_epochs,\n",
    "         report_period=5\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Accuracy: %f\" % (correct / total))\n",
    "print(\"Total:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))\n",
    "print(\"Total:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77aa5c7a8032890130e9a412da4828609b1dfa25c62620094c03af7a5cea44b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
