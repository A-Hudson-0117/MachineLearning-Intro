{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5 Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## define model and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_for_validation = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(feature, w5, w4, w3, w2, w1, b):\n",
    "    return feature[4] * w5 + feature[3] * w4 + feature[2] * w3 + feature[1] * w2 + feature[0] * w1 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(predicted, actual):\n",
    "    squared_diffs = (predicted - actual)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 0.1, 0.01, 0.001, 0.0001, 1e-05]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_to_learn_at = [1/x for x in [5, 10, 100, 1000, 10000, 100000]]\n",
    "rates_to_learn_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0  13300000  7420         4          2        3      yes        no       no   \n",
       "1  12250000  8960         4          4        4      yes        no       no   \n",
       "2  12250000  9960         3          2        2      yes        no      yes   \n",
       "3  12215000  7500         4          2        2      yes        no      yes   \n",
       "4  11410000  7420         4          1        2      yes       yes      yes   \n",
       "\n",
       "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0              no             yes        2      yes        furnished  \n",
       "1              no             yes        3       no        furnished  \n",
       "2              no              no        2      yes   semi-furnished  \n",
       "3              no             yes        3      yes        furnished  \n",
       "4              no             yes        2       no        furnished  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df = pd.DataFrame(pd.read_csv('Housing.csv'))\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (545, 13)\n",
      "features are: ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus']\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape = {np.shape(housing_df)}\")\n",
    "\n",
    "# creates a list of all variables from the column names\n",
    "feature_list = list( housing_df.columns )\n",
    "\n",
    "print(f\"features are: {feature_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary vars = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
      "furnish vars = ['furnishingstatus']\n",
      "value vars = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n"
     ]
    }
   ],
   "source": [
    "# Maps to turn categorys into numbers \n",
    "def boolean_map(x):\n",
    "    return x.map({'yes': 1 , 'no': 0})\n",
    "def furnish_map(x):\n",
    "    return x.map({'furnished': 1 , 'semi-furnished': 0.5 , 'unfurnished': 0})\n",
    "\n",
    "# Extracts the yes and no column names\n",
    "binary_vars = [*feature_list[5:10], feature_list[11]]\n",
    "print(f\"binary vars = {binary_vars}\")\n",
    "\n",
    "# Extracts the furnishing column names\n",
    "furnish_vars = [feature_list[12]]\n",
    "print(f\"furnish vars = {furnish_vars}\")\n",
    "\n",
    "# Extracts the column names that are actual values\n",
    "valued_vars = feature_list.copy()\n",
    "[valued_vars.remove( item ) for item in binary_vars]\n",
    "[valued_vars.remove( item ) for item in furnish_vars]\n",
    "print(f\"value vars = {valued_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = housing_df.copy()\n",
    "\n",
    "## scale data\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "x_df[valued_vars] = scaler.fit_transform(x_df[valued_vars])\n",
    "\n",
    "## map text values\n",
    "x_df[binary_vars] = x_df[binary_vars].apply(boolean_map)\n",
    "x_df[furnish_vars] = x_df[furnish_vars].apply(furnish_map)\n",
    "\n",
    "## make y_df\n",
    "y_df = x_df.pop('price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_values = valued_vars.copy()\n",
    "# input_values.remove('price')\n",
    "\n",
    "\n",
    "# x_df = x_df[input_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.046726</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>1.421812</td>\n",
       "      <td>1.378217</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.517692</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.757010</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>5.405809</td>\n",
       "      <td>2.532024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.679409</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.218232</td>\n",
       "      <td>0.047278</td>\n",
       "      <td>1.421812</td>\n",
       "      <td>0.224410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.517692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.083624</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>1.421812</td>\n",
       "      <td>0.224410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.679409</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.046726</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>-0.570187</td>\n",
       "      <td>0.224410</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.517692</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       area  bedrooms  bathrooms   stories  mainroad  guestroom  basement  \\\n",
       "0  1.046726  1.403419   1.421812  1.378217         1          0         0   \n",
       "1  1.757010  1.403419   5.405809  2.532024         1          0         0   \n",
       "2  2.218232  0.047278   1.421812  0.224410         1          0         1   \n",
       "3  1.083624  1.403419   1.421812  0.224410         1          0         1   \n",
       "4  1.046726  1.403419  -0.570187  0.224410         1          1         1   \n",
       "\n",
       "   hotwaterheating  airconditioning   parking  prefarea  furnishingstatus  \n",
       "0                0                1  1.517692         1               1.0  \n",
       "1                0                1  2.679409         0               1.0  \n",
       "2                0                0  1.517692         1               0.5  \n",
       "3                0                1  2.679409         1               1.0  \n",
       "4                0                1  1.517692         0               1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.566365\n",
       "1    4.004484\n",
       "2    4.004484\n",
       "3    3.985755\n",
       "4    3.554979\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>parking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.046726</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>1.421812</td>\n",
       "      <td>1.378217</td>\n",
       "      <td>1.517692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.757010</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>5.405809</td>\n",
       "      <td>2.532024</td>\n",
       "      <td>2.679409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.218232</td>\n",
       "      <td>0.047278</td>\n",
       "      <td>1.421812</td>\n",
       "      <td>0.224410</td>\n",
       "      <td>1.517692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.083624</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>1.421812</td>\n",
       "      <td>0.224410</td>\n",
       "      <td>2.679409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.046726</td>\n",
       "      <td>1.403419</td>\n",
       "      <td>-0.570187</td>\n",
       "      <td>0.224410</td>\n",
       "      <td>1.517692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       area  bedrooms  bathrooms   stories   parking\n",
       "0  1.046726  1.403419   1.421812  1.378217  1.517692\n",
       "1  1.757010  1.403419   5.405809  2.532024  2.679409\n",
       "2  2.218232  0.047278   1.421812  0.224410  1.517692\n",
       "3  1.083624  1.403419   1.421812  0.224410  2.679409\n",
       "4  1.046726  1.403419  -0.570187  0.224410  1.517692"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unwanted data\n",
    "for item in [*binary_vars, *furnish_vars] :\n",
    "    x_df.pop(item)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## convert data to tensor and test accessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data frame to tensor\n",
    "\n",
    "x = torch.tensor(x_df.values)\n",
    "y = torch.tensor(y_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([545, 5])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0467,  1.4034,  ...,  1.3782,  1.5177],\n",
       "        [ 1.7570,  1.4034,  ...,  2.5320,  2.6794],\n",
       "        ...,\n",
       "        [-1.0334,  0.0473,  ..., -0.9294, -0.8057],\n",
       "        [-0.5998,  0.0473,  ...,  0.2244, -0.8057]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(\"\\n\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([545])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5664e+00,  4.0045e+00,  4.0045e+00,  3.9858e+00,  3.5550e+00,\n",
       "         3.2553e+00,  2.8807e+00,  2.8807e+00,  2.7309e+00,  2.6934e+00,\n",
       "         2.6934e+00,  2.6297e+00,  2.4312e+00,  2.3938e+00,  2.3938e+00,\n",
       "         2.3188e+00,  2.3188e+00,  2.2439e+00,  2.2065e+00,  2.1877e+00,\n",
       "         2.1315e+00,  2.0941e+00,  2.0754e+00,  2.0754e+00,  2.0379e+00,\n",
       "         2.0192e+00,  1.9780e+00,  1.9443e+00,  1.9443e+00,  1.9443e+00,\n",
       "         1.9443e+00,  1.9443e+00,  1.8881e+00,  1.8319e+00,  1.7944e+00,\n",
       "         1.7735e+00,  1.7532e+00,  1.7195e+00,  1.7101e+00,  1.6820e+00,\n",
       "         1.6633e+00,  1.6446e+00,  1.5697e+00,  1.5697e+00,  1.4947e+00,\n",
       "         1.4947e+00,  1.4760e+00,  1.4573e+00,  1.4386e+00,  1.4198e+00,\n",
       "         1.4198e+00,  1.4198e+00,  1.3824e+00,  1.3824e+00,  1.3824e+00,\n",
       "         1.3824e+00,  1.3786e+00,  1.3262e+00,  1.3075e+00,  1.3075e+00,\n",
       "         1.2700e+00,  1.2325e+00,  1.2325e+00,  1.2138e+00,  1.1951e+00,\n",
       "         1.1576e+00,  1.1576e+00,  1.1389e+00,  1.1202e+00,  1.0827e+00,\n",
       "         1.0827e+00,  1.0640e+00,  1.0452e+00,  1.0265e+00,  1.0078e+00,\n",
       "         1.0078e+00,  1.0078e+00,  1.0078e+00,  1.0078e+00,  1.0078e+00,\n",
       "         9.9655e-01,  9.8906e-01,  9.8906e-01,  9.7033e-01,  9.3287e-01,\n",
       "         9.3287e-01,  9.3287e-01,  9.1414e-01,  9.1414e-01,  8.9541e-01,\n",
       "         8.9541e-01,  8.8417e-01,  8.7668e-01,  8.2049e-01,  8.2049e-01,\n",
       "         8.2049e-01,  8.2049e-01,  8.2049e-01,  8.1675e-01,  8.0176e-01,\n",
       "         7.8303e-01,  7.8303e-01,  7.6430e-01,  7.6430e-01,  7.6430e-01,\n",
       "         7.4557e-01,  7.4557e-01,  7.2684e-01,  7.1748e-01,  7.0812e-01,\n",
       "         7.0812e-01,  7.0812e-01,  7.0437e-01,  7.0437e-01,  6.7066e-01,\n",
       "         6.7066e-01,  6.7066e-01,  6.3320e-01,  6.3320e-01,  6.3320e-01,\n",
       "         6.3320e-01,  6.3320e-01,  6.3320e-01,  6.3320e-01,  6.3320e-01,\n",
       "         6.2945e-01,  5.9574e-01,  5.9574e-01,  5.9199e-01,  5.9199e-01,\n",
       "         5.8825e-01,  5.5828e-01,  5.5828e-01,  5.5828e-01,  5.5453e-01,\n",
       "         5.3955e-01,  5.2082e-01,  5.2082e-01,  5.2082e-01,  5.2082e-01,\n",
       "         5.2082e-01,  4.7400e-01,  4.4590e-01,  4.4590e-01,  4.4590e-01,\n",
       "         4.4590e-01,  4.4590e-01,  4.4590e-01,  4.4590e-01,  4.4590e-01,\n",
       "         4.4590e-01,  4.2717e-01,  4.2717e-01,  4.0845e-01,  4.0845e-01,\n",
       "         4.0845e-01,  4.0470e-01,  3.8972e-01,  3.8972e-01,  3.7099e-01,\n",
       "         3.7099e-01,  3.7099e-01,  3.7099e-01,  3.5226e-01,  3.3353e-01,\n",
       "         3.2978e-01,  2.9607e-01,  2.7734e-01,  2.5861e-01,  2.5861e-01,\n",
       "         2.5861e-01,  2.5861e-01,  2.5861e-01,  2.5861e-01,  2.5861e-01,\n",
       "         2.5861e-01,  2.5861e-01,  2.5486e-01,  2.4737e-01,  2.3988e-01,\n",
       "         2.3988e-01,  2.3988e-01,  2.0242e-01,  2.0242e-01,  1.8369e-01,\n",
       "         1.8369e-01,  1.8369e-01,  1.8369e-01,  1.6496e-01,  1.4623e-01,\n",
       "         1.4623e-01,  1.4623e-01,  1.4623e-01,  1.4249e-01,  1.2750e-01,\n",
       "         1.0878e-01,  1.0878e-01,  1.0128e-01,  9.0046e-02,  7.5062e-02,\n",
       "         7.1316e-02,  7.1316e-02,  7.1316e-02,  7.1316e-02,  7.1316e-02,\n",
       "         7.1316e-02,  7.1316e-02,  7.1316e-02,  7.1316e-02,  7.1316e-02,\n",
       "         7.1316e-02,  7.1316e-02,  6.7571e-02,  6.7571e-02,  5.2587e-02,\n",
       "         3.3858e-02,  3.3858e-02,  3.3858e-02,  3.3858e-02,  1.5128e-02,\n",
       "         1.5128e-02,  1.4489e-04, -3.6010e-03, -3.6010e-03, -3.6010e-03,\n",
       "        -7.3469e-03, -4.1060e-02, -4.1060e-02, -4.1060e-02, -4.1060e-02,\n",
       "        -4.1060e-02, -4.1060e-02, -5.9789e-02, -7.8518e-02, -7.8518e-02,\n",
       "        -7.8518e-02, -7.8518e-02, -7.8518e-02, -8.2264e-02, -9.7248e-02,\n",
       "        -9.7248e-02, -1.1598e-01, -1.1598e-01, -1.1598e-01, -1.1598e-01,\n",
       "        -1.1598e-01, -1.1598e-01, -1.1598e-01, -1.1972e-01, -1.1972e-01,\n",
       "        -1.3471e-01, -1.3471e-01, -1.3471e-01, -1.3471e-01, -1.5344e-01,\n",
       "        -1.5344e-01, -1.5344e-01, -1.5344e-01, -1.5344e-01, -1.5718e-01,\n",
       "        -1.5718e-01, -1.5718e-01, -1.7217e-01, -1.9089e-01, -1.9089e-01,\n",
       "        -1.9464e-01, -1.9464e-01, -1.9464e-01, -2.0588e-01, -2.0962e-01,\n",
       "        -2.2835e-01, -2.2835e-01, -2.2835e-01, -2.2835e-01, -2.2835e-01,\n",
       "        -2.3959e-01, -2.4708e-01, -2.4708e-01, -2.6207e-01, -2.6581e-01,\n",
       "        -2.6581e-01, -2.6581e-01, -2.6581e-01, -2.6581e-01, -2.6581e-01,\n",
       "        -2.8454e-01, -2.8454e-01, -3.0327e-01, -3.0327e-01, -3.0327e-01,\n",
       "        -3.0327e-01, -3.0327e-01, -3.0327e-01, -3.0327e-01, -3.0327e-01,\n",
       "        -3.0327e-01, -3.0327e-01, -3.0327e-01, -3.0327e-01, -3.0327e-01,\n",
       "        -3.0327e-01, -3.0327e-01, -3.0327e-01, -3.0327e-01, -3.0702e-01,\n",
       "        -3.0702e-01, -3.2200e-01, -3.2200e-01, -3.2200e-01, -3.4073e-01,\n",
       "        -3.4073e-01, -3.4448e-01, -3.5759e-01, -3.5946e-01, -3.5946e-01,\n",
       "        -3.5946e-01, -3.7819e-01, -3.7819e-01, -3.7819e-01, -3.7819e-01,\n",
       "        -3.7819e-01, -3.9692e-01, -3.9692e-01, -3.9692e-01, -4.0628e-01,\n",
       "        -4.0628e-01, -4.1565e-01, -4.1565e-01, -4.1565e-01, -4.1565e-01,\n",
       "        -4.1565e-01, -4.5311e-01, -4.5311e-01, -4.5311e-01, -4.5311e-01,\n",
       "        -4.5311e-01, -4.5311e-01, -4.5311e-01, -4.7184e-01, -4.7184e-01,\n",
       "        -4.9056e-01, -4.9056e-01, -4.9056e-01, -4.9056e-01, -4.9056e-01,\n",
       "        -4.9056e-01, -4.9056e-01, -4.9806e-01, -5.0929e-01, -5.2802e-01,\n",
       "        -5.2802e-01, -5.2802e-01, -5.2802e-01, -5.2802e-01, -5.2802e-01,\n",
       "        -5.3177e-01, -5.3177e-01, -5.3177e-01, -5.4675e-01, -5.6548e-01,\n",
       "        -5.6548e-01, -5.6548e-01, -5.6548e-01, -5.6548e-01, -5.6923e-01,\n",
       "        -5.6923e-01, -5.8421e-01, -5.8421e-01, -5.8421e-01, -5.8421e-01,\n",
       "        -6.0294e-01, -6.0294e-01, -6.0294e-01, -6.0294e-01, -6.0294e-01,\n",
       "        -6.0294e-01, -6.0294e-01, -6.0294e-01, -6.0294e-01, -6.0669e-01,\n",
       "        -6.2167e-01, -6.2167e-01, -6.4040e-01, -6.4040e-01, -6.4040e-01,\n",
       "        -6.4040e-01, -6.5913e-01, -6.7786e-01, -6.7786e-01, -6.7786e-01,\n",
       "        -6.7786e-01, -6.7786e-01, -6.7786e-01, -6.7786e-01, -6.7786e-01,\n",
       "        -6.7786e-01, -6.7786e-01, -6.7786e-01, -6.7786e-01, -6.7786e-01,\n",
       "        -6.7786e-01, -6.7786e-01, -6.7786e-01, -6.7786e-01, -6.8160e-01,\n",
       "        -6.9659e-01, -6.9659e-01, -6.9659e-01, -7.1532e-01, -7.1532e-01,\n",
       "        -7.1532e-01, -7.1532e-01, -7.1532e-01, -7.1532e-01, -7.1906e-01,\n",
       "        -7.3405e-01, -7.3405e-01, -7.3405e-01, -7.5278e-01, -7.5278e-01,\n",
       "        -7.5278e-01, -7.5278e-01, -7.5278e-01, -7.5278e-01, -7.5278e-01,\n",
       "        -7.5278e-01, -7.5652e-01, -7.6776e-01, -7.7151e-01, -7.7151e-01,\n",
       "        -7.9023e-01, -7.9023e-01, -7.9023e-01, -7.9023e-01, -7.9023e-01,\n",
       "        -7.9023e-01, -7.9023e-01, -7.9023e-01, -8.0896e-01, -8.0896e-01,\n",
       "        -8.2020e-01, -8.2769e-01, -8.2769e-01, -8.2769e-01, -8.2769e-01,\n",
       "        -8.6515e-01, -8.6515e-01, -8.6515e-01, -8.6515e-01, -8.6515e-01,\n",
       "        -8.6515e-01, -8.6515e-01, -8.6515e-01, -8.6515e-01, -8.6890e-01,\n",
       "        -8.7639e-01, -8.8182e-01, -8.8388e-01, -8.8388e-01, -8.8388e-01,\n",
       "        -8.9886e-01, -9.0261e-01, -9.0261e-01, -9.0261e-01, -9.0261e-01,\n",
       "        -9.2134e-01, -9.4007e-01, -9.4007e-01, -9.4007e-01, -9.4007e-01,\n",
       "        -9.4007e-01, -9.4007e-01, -9.4007e-01, -9.4382e-01, -9.5880e-01,\n",
       "        -9.6629e-01, -9.7753e-01, -9.7753e-01, -9.7753e-01, -9.7753e-01,\n",
       "        -9.7753e-01, -9.7753e-01, -9.7753e-01, -9.7753e-01, -1.0150e+00,\n",
       "        -1.0150e+00, -1.0150e+00, -1.0150e+00, -1.0244e+00, -1.0337e+00,\n",
       "        -1.0337e+00, -1.0337e+00, -1.0524e+00, -1.0524e+00, -1.0899e+00,\n",
       "        -1.0899e+00, -1.1086e+00, -1.1274e+00, -1.1274e+00, -1.1274e+00,\n",
       "        -1.1274e+00, -1.1274e+00, -1.1274e+00, -1.1274e+00, -1.1311e+00,\n",
       "        -1.1311e+00, -1.1573e+00, -1.1648e+00, -1.1648e+00, -1.1648e+00,\n",
       "        -1.2023e+00, -1.2023e+00, -1.2023e+00, -1.2210e+00, -1.2210e+00,\n",
       "        -1.2397e+00, -1.2397e+00, -1.2397e+00, -1.2397e+00, -1.2397e+00,\n",
       "        -1.2397e+00, -1.2622e+00, -1.2772e+00, -1.2772e+00, -1.2772e+00,\n",
       "        -1.2959e+00, -1.3147e+00, -1.3334e+00, -1.3334e+00, -1.3334e+00,\n",
       "        -1.3521e+00, -1.3559e+00, -1.4083e+00, -1.4270e+00, -1.4270e+00,\n",
       "        -1.4270e+00, -1.5020e+00, -1.5394e+00, -1.5394e+00, -1.5581e+00,\n",
       "        -1.5769e+00, -1.6051e+00, -1.6143e+00, -1.6143e+00, -1.6143e+00],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(\"\\n\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7570, dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4034, dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0467, 1.4034, 1.4218, 1.3782, 1.5177], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0467, 1.4034, 1.4218, 1.3782, 1.5177], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 545])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = x.transpose(-2, 1)\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4034,  1.4034,  0.0473,  1.4034,  1.4034,  0.0473,  1.4034,\n",
       "         2.7596,  1.4034,  0.0473,  0.0473,  1.4034,  1.4034,  1.4034,\n",
       "         0.0473,  1.4034,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         2.7596,  1.4034,  0.0473,  0.0473,  1.4034,  0.0473,  2.7596,\n",
       "         0.0473,  0.0473,  1.4034,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  0.0473,\n",
       "         1.4034,  1.4034,  1.4034,  0.0473,  0.0473, -1.3089,  1.4034,\n",
       "         1.4034,  0.0473,  0.0473, -1.3089,  0.0473,  0.0473,  1.4034,\n",
       "         0.0473,  1.4034,  0.0473, -1.3089,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  2.7596,  0.0473,\n",
       "        -1.3089,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         4.1157,  0.0473, -1.3089,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  1.4034,  2.7596,  1.4034,  0.0473, -1.3089,\n",
       "         0.0473,  0.0473,  1.4034,  0.0473,  1.4034,  2.7596,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  1.4034,\n",
       "         0.0473,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473, -1.3089,\n",
       "         1.4034,  1.4034,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,\n",
       "        -1.3089,  1.4034,  0.0473,  0.0473, -1.3089,  0.0473,  1.4034,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473, -1.3089,\n",
       "         1.4034,  0.0473,  0.0473, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  1.4034,  1.4034, -1.3089,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  1.4034,  1.4034, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  1.4034, -1.3089,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473, -1.3089,  0.0473,\n",
       "         1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  1.4034, -1.3089,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  1.4034, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "        -1.3089,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  2.7596,  0.0473,\n",
       "         1.4034,  1.4034, -1.3089, -1.3089, -1.3089,  0.0473, -1.3089,\n",
       "        -1.3089, -1.3089,  0.0473,  1.4034, -1.3089,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473, -1.3089,  1.4034,  0.0473,  1.4034, -1.3089,\n",
       "         1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  0.0473, -1.3089,  0.0473,  0.0473, -1.3089,\n",
       "        -1.3089,  1.4034,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  1.4034,  0.0473, -1.3089, -1.3089,\n",
       "         0.0473,  1.4034, -1.3089,  1.4034,  2.7596, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  2.7596,\n",
       "         1.4034, -1.3089,  0.0473, -1.3089,  0.0473, -1.3089, -1.3089,\n",
       "         0.0473, -1.3089, -1.3089, -1.3089, -1.3089, -1.3089, -1.3089,\n",
       "         0.0473,  0.0473, -1.3089, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473, -1.3089, -1.3089,  0.0473,  1.4034, -1.3089,\n",
       "        -1.3089,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  4.1157, -1.3089, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "        -1.3089,  0.0473, -1.3089, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473, -1.3089,  1.4034,  1.4034,\n",
       "        -1.3089, -1.3089, -1.3089,  0.0473,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473, -1.3089,  1.4034, -1.3089,  1.4034,  0.0473,  1.4034,\n",
       "         1.4034, -1.3089,  0.0473,  0.0473, -1.3089, -1.3089,  1.4034,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473, -2.6650, -1.3089, -1.3089,\n",
       "        -1.3089,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473, -1.3089, -1.3089, -1.3089,\n",
       "         0.0473, -1.3089, -1.3089, -1.3089,  0.0473,  0.0473, -1.3089,\n",
       "        -1.3089,  0.0473,  0.0473,  1.4034, -1.3089,  1.4034, -1.3089,\n",
       "         0.0473, -1.3089,  0.0473,  1.4034,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473, -1.3089, -1.3089, -1.3089,  1.4034,  1.4034,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,\n",
       "        -1.3089, -1.3089,  0.0473,  0.0473,  0.0473, -1.3089,  0.0473,\n",
       "        -1.3089,  0.0473, -1.3089, -1.3089, -1.3089, -1.3089,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473,  0.0473, -1.3089, -1.3089,\n",
       "        -1.3089, -1.3089, -1.3089, -1.3089,  0.0473,  1.4034, -1.3089,\n",
       "        -1.3089, -1.3089, -1.3089, -2.6650,  0.0473,  0.0473,  0.0473,\n",
       "        -1.3089,  0.0473,  1.4034, -1.3089,  2.7596,  0.0473, -1.3089,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  0.0473],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t[1][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4034,  1.4034,  0.0473,  1.4034,  1.4034,  0.0473,  1.4034,\n",
       "         2.7596,  1.4034,  0.0473,  0.0473,  1.4034,  1.4034,  1.4034,\n",
       "         0.0473,  1.4034,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         2.7596,  1.4034,  0.0473,  0.0473,  1.4034,  0.0473,  2.7596,\n",
       "         0.0473,  0.0473,  1.4034,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  0.0473,\n",
       "         1.4034,  1.4034,  1.4034,  0.0473,  0.0473, -1.3089,  1.4034,\n",
       "         1.4034,  0.0473,  0.0473, -1.3089,  0.0473,  0.0473,  1.4034,\n",
       "         0.0473,  1.4034,  0.0473, -1.3089,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  2.7596,  0.0473,\n",
       "        -1.3089,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         4.1157,  0.0473, -1.3089,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  1.4034,  2.7596,  1.4034,  0.0473, -1.3089,\n",
       "         0.0473,  0.0473,  1.4034,  0.0473,  1.4034,  2.7596,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  1.4034,\n",
       "         0.0473,  1.4034,  0.0473,  0.0473,  0.0473,  0.0473, -1.3089,\n",
       "         1.4034,  1.4034,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,\n",
       "        -1.3089,  1.4034,  0.0473,  0.0473, -1.3089,  0.0473,  1.4034,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473, -1.3089,\n",
       "         1.4034,  0.0473,  0.0473, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  1.4034,  1.4034, -1.3089,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  1.4034,  1.4034, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  1.4034, -1.3089,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473, -1.3089,  0.0473,\n",
       "         1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  1.4034, -1.3089,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  1.4034, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "        -1.3089,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473,  0.0473,  1.4034,  0.0473,  0.0473,  2.7596,  0.0473,\n",
       "         1.4034,  1.4034, -1.3089, -1.3089, -1.3089,  0.0473, -1.3089,\n",
       "        -1.3089, -1.3089,  0.0473,  1.4034, -1.3089,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473, -1.3089,  1.4034,  0.0473,  1.4034, -1.3089,\n",
       "         1.4034,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  1.4034,  0.0473, -1.3089,  0.0473,  0.0473, -1.3089,\n",
       "        -1.3089,  1.4034,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  1.4034,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  1.4034,  0.0473, -1.3089, -1.3089,\n",
       "         0.0473,  1.4034, -1.3089,  1.4034,  2.7596, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  2.7596,\n",
       "         1.4034, -1.3089,  0.0473, -1.3089,  0.0473, -1.3089, -1.3089,\n",
       "         0.0473, -1.3089, -1.3089, -1.3089, -1.3089, -1.3089, -1.3089,\n",
       "         0.0473,  0.0473, -1.3089, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473, -1.3089, -1.3089,  0.0473,  1.4034, -1.3089,\n",
       "        -1.3089,  0.0473,  0.0473,  0.0473,  1.4034,  0.0473,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  4.1157, -1.3089, -1.3089,  0.0473,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "        -1.3089,  0.0473, -1.3089, -1.3089,  0.0473,  0.0473,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473, -1.3089,  1.4034,  1.4034,\n",
       "        -1.3089, -1.3089, -1.3089,  0.0473,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473, -1.3089,  1.4034, -1.3089,  1.4034,  0.0473,  1.4034,\n",
       "         1.4034, -1.3089,  0.0473,  0.0473, -1.3089, -1.3089,  1.4034,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473, -2.6650, -1.3089, -1.3089,\n",
       "        -1.3089,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473,  0.0473,  0.0473,  0.0473, -1.3089, -1.3089, -1.3089,\n",
       "         0.0473, -1.3089, -1.3089, -1.3089,  0.0473,  0.0473, -1.3089,\n",
       "        -1.3089,  0.0473,  0.0473,  1.4034, -1.3089,  1.4034, -1.3089,\n",
       "         0.0473, -1.3089,  0.0473,  1.4034,  0.0473, -1.3089,  0.0473,\n",
       "         0.0473, -1.3089, -1.3089, -1.3089,  1.4034,  1.4034,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473, -1.3089,  0.0473, -1.3089,\n",
       "        -1.3089, -1.3089,  0.0473,  0.0473,  0.0473, -1.3089,  0.0473,\n",
       "        -1.3089,  0.0473, -1.3089, -1.3089, -1.3089, -1.3089,  0.0473,\n",
       "         0.0473, -1.3089,  0.0473,  0.0473,  0.0473, -1.3089, -1.3089,\n",
       "        -1.3089, -1.3089, -1.3089, -1.3089,  0.0473,  1.4034, -1.3089,\n",
       "        -1.3089, -1.3089, -1.3089, -2.6650,  0.0473,  0.0473,  0.0473,\n",
       "        -1.3089,  0.0473,  1.4034, -1.3089,  2.7596,  0.0473, -1.3089,\n",
       "        -1.3089, -1.3089,  0.0473, -1.3089,  0.0473,  0.0473],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t[:][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:\n",
      "tensor([392, 113, 199, 146,  25, 372, 401, 226, 105, 496, 471, 495, 424,\n",
      "        141, 257, 337, 244, 156, 234, 216, 214,  31, 159, 220,  47, 359,\n",
      "        529, 266,  95, 264,  70, 188, 206, 175, 248, 370, 295, 346,  14,\n",
      "        174, 341, 391, 106, 240, 542, 164,  17, 317, 343,  68, 283, 270,\n",
      "        137, 322,  22, 170, 432, 184, 291,  33, 415, 536, 380, 475, 324,\n",
      "        455, 509, 449, 420, 451, 313, 520, 528, 437, 368, 506, 396, 442,\n",
      "        296, 260,  30, 428,  98, 356, 409,  65, 394, 262, 321, 441, 473,\n",
      "        186, 461,  92,  97, 300, 282, 134, 298,  29, 480, 363, 497,  27,\n",
      "        486, 417, 385, 128, 397, 143, 325, 446,  93, 178,  58, 124, 114,\n",
      "        485,   3,  72, 163, 147, 187, 393, 478, 243, 407, 229,  42,  84,\n",
      "        204, 479, 232, 519,  52,  54, 468, 145, 285, 152, 129,  75, 350,\n",
      "         82, 221, 185, 474,  62, 402, 526, 203, 290, 273, 457,  45, 459,\n",
      "        263, 265, 150, 366, 414, 167, 112, 399,  13, 127, 403, 389, 319,\n",
      "        332, 118, 386, 211, 133, 280, 483, 416,  79, 182, 490, 109, 195,\n",
      "         80, 284,  56, 469,   1, 315, 501, 241, 304, 438, 427, 138,   8,\n",
      "        460, 222, 434, 450, 338,  15, 169, 172, 533, 107, 537, 544, 165,\n",
      "        487, 330, 303, 320, 516, 230, 492, 523, 212,   0, 517, 521, 227,\n",
      "         96, 202, 440, 121, 272, 493,  36, 349, 439, 418, 527, 333, 507,\n",
      "        335,  11, 177, 348, 261, 499, 236, 275, 299, 382, 191, 388, 245,\n",
      "        142, 289, 423, 276,  24, 498, 354, 239, 287,  57, 540, 413, 510,\n",
      "        445, 411, 256, 209, 267, 340, 374, 327, 452,  23, 116, 311,   5,\n",
      "          2, 502, 404, 381, 511, 525, 235, 344, 443,  28,  76,  73,  94,\n",
      "        367, 358, 456,  59, 454,  20, 477, 379, 530,  50, 126, 139, 308,\n",
      "        288, 193,  81, 110,  37, 180,  71, 426,  99, 357, 326, 421, 532,\n",
      "        481,  91, 233, 200, 328, 179, 277, 268, 130, 219, 425, 293, 136,\n",
      "        120, 494, 431, 329, 155, 131, 135, 247, 132,  69, 433,  43,   7,\n",
      "        436, 223, 334, 429, 242, 162, 218, 500, 215,  64,  55, 189, 198,\n",
      "        306, 398,  83, 166, 310, 430, 390, 535, 251,  86, 336, 352, 271,\n",
      "        307,  89, 410, 100, 408, 225,  21, 192,  48, 297, 342,   9, 176,\n",
      "        345,  67, 331,  41,  44, 309, 228, 160, 259, 111, 286, 355, 364,\n",
      "        316, 406, 522, 476,  60, 484,  46, 353, 376, 254,  88, 422,  10,\n",
      "        504, 360, 115,  53, 462,  39, 194, 482, 377, 383, 125, 103, 104,\n",
      "        205, 217, 301, 375, 181, 412, 302, 253, 231, 274, 463, 197, 539,\n",
      "        294, 531, 305, 444, 246,  66, 101])\n",
      "\n",
      "validation:\n",
      "tensor([387, 323, 538,  38, 365, 171, 362,  78, 190, 488, 371, 292,  19,\n",
      "        470, 447, 168, 513, 250,  34, 314, 255, 278, 464, 213, 117,  40,\n",
      "        102, 151, 489, 173, 534, 148, 238, 465, 419, 161, 503,  18,  32,\n",
      "        318, 224, 183,  85, 279, 249, 122, 467,  26, 524,  16, 108, 237,\n",
      "        378, 347, 252,  61,  74, 144, 351, 369,  35,  63, 491, 196,  90,\n",
      "        518,  77,  51, 140, 269,   6, 201, 515, 207, 514, 543,  87, 154,\n",
      "        541, 373, 435,   4, 149, 400, 505, 512,  49, 458, 466, 158, 281,\n",
      "        395, 258, 508,  12, 339, 119, 405, 453, 157, 153, 361, 472, 208,\n",
      "        448, 210, 123, 312, 384])\n"
     ]
    }
   ],
   "source": [
    "n_samples = x.shape[0]\n",
    "n_val = int(percent_for_validation * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "print(f\"training:\\n{train_indices}\")\n",
    "print()\n",
    "print(f\"validation:\\n{val_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = x[train_indices]\n",
    "training_y  = y [train_indices]\n",
    "\n",
    "validation_x = x[val_indices]\n",
    "validation_y  = y[val_indices]\n",
    "\n",
    "# training_un   = 0.1 * training_feature\n",
    "# validation_un = 0.1 * validation_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, \n",
    "                  train_x, val_x,\n",
    "                  train_y, val_y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_predicted = model(train_x.transpose(-2, 1), *params)\n",
    "        train_loss = loss_function(train_predicted, train_y)\n",
    "\n",
    "        # For the valudation step\n",
    "        with torch.no_grad(): \n",
    "            val_predicted = model(val_x.transpose(-2, 1), *params)\n",
    "            val_loss = loss_function(val_predicted, val_y)\n",
    "            assert val_loss.requires_grad == False # <2>\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.5f},\"\n",
    "                  f\" Validation loss {val_loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Learning rate is 0.2 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 0.70746, Validation loss 0.57077\n",
      "Epoch 3, Training loss 0.45652, Validation loss 0.47187\n",
      "Epoch 500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48317\n",
      "Loss:\t\t0.48317\n",
      "Paramaters:\t[ 0.1611128   0.25495437  0.30357212  0.04907718  0.36485973 -0.02390866]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.1 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 2.61109, Validation loss 1.89034\n",
      "Epoch 3, Training loss 1.23788, Validation loss 0.90639\n",
      "Epoch 500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48317\n",
      "Loss:\t\t0.48317\n",
      "Paramaters:\t[ 0.16111283  0.25495434  0.30357206  0.04907722  0.36485967 -0.02390865]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.01 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 5.91895, Validation loss 4.40829\n",
      "Epoch 3, Training loss 5.49399, Validation loss 4.07954\n",
      "Epoch 500, Training loss 0.42942, Validation loss 0.48312\n",
      "Epoch 1000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48317\n",
      "Loss:\t\t0.48317\n",
      "Paramaters:\t[ 0.16111349  0.25495365  0.3035714   0.0490779   0.3648589  -0.02390858]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 6.33283, Validation loss 4.72941\n",
      "Epoch 3, Training loss 6.28628, Validation loss 4.69325\n",
      "Epoch 500, Training loss 0.55535, Validation loss 0.48951\n",
      "Epoch 1000, Training loss 0.43577, Validation loss 0.47170\n",
      "Epoch 1500, Training loss 0.43069, Validation loss 0.48007\n",
      "Epoch 2000, Training loss 0.42978, Validation loss 0.48184\n",
      "Epoch 2500, Training loss 0.42952, Validation loss 0.48243\n",
      "Epoch 3000, Training loss 0.42945, Validation loss 0.48274\n",
      "Epoch 3500, Training loss 0.42943, Validation loss 0.48292\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48303\n",
      "Epoch 4500, Training loss 0.42942, Validation loss 0.48309\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48312\n",
      "Loss:\t\t0.48312\n",
      "Paramaters:\t[ 0.16139227  0.25491023  0.30322793  0.04944179  0.36464056 -0.02386857]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.0001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 6.37504, Validation loss 4.76221\n",
      "Epoch 3, Training loss 6.37035, Validation loss 4.75856\n",
      "Epoch 500, Training loss 4.44220, Validation loss 3.27095\n",
      "Epoch 1000, Training loss 3.13577, Validation loss 2.28081\n",
      "Epoch 1500, Training loss 2.25672, Validation loss 1.62920\n",
      "Epoch 2000, Training loss 1.66500, Validation loss 1.20254\n",
      "Epoch 2500, Training loss 1.26646, Validation loss 0.92498\n",
      "Epoch 3000, Training loss 0.99785, Validation loss 0.74594\n",
      "Epoch 3500, Training loss 0.81664, Validation loss 0.63173\n",
      "Epoch 4000, Training loss 0.69424, Validation loss 0.55997\n",
      "Epoch 4500, Training loss 0.61144, Validation loss 0.51582\n",
      "Epoch 5000, Training loss 0.55532, Validation loss 0.48948\n",
      "Loss:\t\t0.48944\n",
      "Paramaters:\t[ 0.34569085  0.35948676  0.32982805  0.22665294  0.41470024 -0.01649503]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 1e-05 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 6.37927, Validation loss 4.76550\n",
      "Epoch 3, Training loss 6.37880, Validation loss 4.76513\n",
      "Epoch 500, Training loss 6.14973, Validation loss 4.58724\n",
      "Epoch 1000, Training loss 5.92822, Validation loss 4.41547\n",
      "Epoch 1500, Training loss 5.71532, Validation loss 4.25063\n",
      "Epoch 2000, Training loss 5.51070, Validation loss 4.09244\n",
      "Epoch 2500, Training loss 5.31403, Validation loss 3.94065\n",
      "Epoch 3000, Training loss 5.12502, Validation loss 3.79499\n",
      "Epoch 3500, Training loss 4.94335, Validation loss 3.65522\n",
      "Epoch 4000, Training loss 4.76875, Validation loss 3.52112\n",
      "Epoch 4500, Training loss 4.60093, Validation loss 3.39244\n",
      "Epoch 5000, Training loss 4.43963, Validation loss 3.26899\n",
      "Loss:\t\t3.26875\n",
      "Paramaters:\t[ 0.87068886  0.8659262   0.853457    0.8419749   0.8752919  -0.00316191]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate in rates_to_learn_at:\n",
    "    print(f\"  Learning rate is {rate} :\\n\"+\"/\\\\\"*15)\n",
    "    \n",
    "    params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
    "\n",
    "    optimizer = optim.SGD([params], lr=rate)\n",
    "\n",
    "    training_loop(\n",
    "        n_epochs = 5000, \n",
    "        optimizer = optimizer,\n",
    "        params = params,\n",
    "        train_x = training_x, \n",
    "        val_x = validation_x, \n",
    "        train_y = training_y,\n",
    "        val_y = validation_y\n",
    "    )\n",
    "    \n",
    "    val_predicted = model(validation_x.transpose(-2, 1), *params)\n",
    "    val_loss = loss_function(val_predicted, validation_y)\n",
    "    \n",
    "    print(f\"Loss:\\t\\t{val_loss:.5f}\")\n",
    "    print(f\"Paramaters:\\t{params.detach().numpy()}\")\n",
    "        \n",
    "    print('\\n'+\"--\"*30+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Learning rate is 0.2 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 3.74830, Validation loss 2.82888\n",
      "Epoch 3, Training loss 1.87967, Validation loss 1.40523\n",
      "Epoch 500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2500, Training loss 0.42942, Validation loss 0.48325\n",
      "Epoch 3000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48356\n",
      "Epoch 4500, Training loss 0.42954, Validation loss 0.47967\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48339\n",
      "Loss:\t\t0.48298\n",
      "Paramaters:\t[ 0.16128545  0.25512707  0.3037455   0.04924798  0.36503342 -0.02377262]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.1 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 4.95486, Validation loss 3.70866\n",
      "Epoch 3, Training loss 3.72719, Validation loss 2.76597\n",
      "Epoch 500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1500, Training loss 0.42942, Validation loss 0.48316\n",
      "Epoch 2000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3000, Training loss 0.42942, Validation loss 0.48323\n",
      "Epoch 3500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48315\n",
      "Epoch 4500, Training loss 0.42942, Validation loss 0.48315\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48317\n",
      "Loss:\t\t0.48317\n",
      "Paramaters:\t[ 0.16111277  0.2549544   0.30357215  0.04907716  0.36485976 -0.02390871]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.01 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 6.22743, Validation loss 4.65216\n",
      "Epoch 3, Training loss 6.07736, Validation loss 4.54019\n",
      "Epoch 500, Training loss 0.42942, Validation loss 0.48306\n",
      "Epoch 1000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 1500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 2500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 3500, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48317\n",
      "Epoch 4500, Training loss 0.42942, Validation loss 0.48316\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48317\n",
      "Loss:\t\t0.48317\n",
      "Paramaters:\t[ 0.16111279  0.2549544   0.30357212  0.04907716  0.36485976 -0.02390849]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 6.36441, Validation loss 4.75441\n",
      "Epoch 3, Training loss 6.34911, Validation loss 4.74298\n",
      "Epoch 500, Training loss 1.70937, Validation loss 1.26479\n",
      "Epoch 1000, Training loss 0.58574, Validation loss 0.52054\n",
      "Epoch 1500, Training loss 0.44625, Validation loss 0.47436\n",
      "Epoch 2000, Training loss 0.43292, Validation loss 0.47962\n",
      "Epoch 2500, Training loss 0.43012, Validation loss 0.48093\n",
      "Epoch 3000, Training loss 0.42951, Validation loss 0.48211\n",
      "Epoch 3500, Training loss 0.42942, Validation loss 0.48284\n",
      "Epoch 4000, Training loss 0.42942, Validation loss 0.48310\n",
      "Epoch 4500, Training loss 0.42942, Validation loss 0.48316\n",
      "Epoch 5000, Training loss 0.42942, Validation loss 0.48317\n",
      "Loss:\t\t0.48317\n",
      "Paramaters:\t[ 0.16111244  0.25495207  0.30357027  0.04908155  0.36485878 -0.02390848]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 0.0001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 6.37821, Validation loss 4.76472\n",
      "Epoch 3, Training loss 6.37668, Validation loss 4.76357\n",
      "Epoch 500, Training loss 5.65136, Validation loss 4.21703\n",
      "Epoch 1000, Training loss 4.98939, Validation loss 3.71196\n",
      "Epoch 1500, Training loss 4.38800, Validation loss 3.25436\n",
      "Epoch 2000, Training loss 3.84208, Validation loss 2.84132\n",
      "Epoch 2500, Training loss 3.34751, Validation loss 2.46955\n",
      "Epoch 3000, Training loss 2.90092, Validation loss 2.13637\n",
      "Epoch 3500, Training loss 2.49958, Validation loss 1.83958\n",
      "Epoch 4000, Training loss 2.14123, Validation loss 1.57733\n",
      "Epoch 4500, Training loss 1.82391, Validation loss 1.34801\n",
      "Epoch 5000, Training loss 1.54585, Validation loss 1.15012\n",
      "Loss:\t\t1.14976\n",
      "Paramaters:\t[ 0.5551984   0.5560366   0.558271    0.55119187  0.563107   -0.0236312 ]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Learning rate is 1e-05 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 1, Training loss 6.37974, Validation loss 4.76586\n",
      "Epoch 2, Training loss 6.37959, Validation loss 4.76575\n",
      "Epoch 3, Training loss 6.37944, Validation loss 4.76563\n",
      "Epoch 500, Training loss 6.30357, Validation loss 4.70893\n",
      "Epoch 1000, Training loss 6.22805, Validation loss 4.65241\n",
      "Epoch 1500, Training loss 6.15310, Validation loss 4.59623\n",
      "Epoch 2000, Training loss 6.07902, Validation loss 4.54063\n",
      "Epoch 2500, Training loss 6.00555, Validation loss 4.48543\n",
      "Epoch 3000, Training loss 5.93260, Validation loss 4.43052\n",
      "Epoch 3500, Training loss 5.86016, Validation loss 4.37589\n",
      "Epoch 4000, Training loss 5.78822, Validation loss 4.32151\n",
      "Epoch 4500, Training loss 5.71684, Validation loss 4.26740\n",
      "Epoch 5000, Training loss 5.64596, Validation loss 4.21351\n",
      "Loss:\t\t4.21341\n",
      "Paramaters:\t[ 0.9504164   0.9504156   0.9504212   0.9504037   0.950482   -0.03192371]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate in rates_to_learn_at:\n",
    "    print(f\"  Learning rate is {rate} :\\n\"+\"/\\\\\"*15)\n",
    "    \n",
    "    params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([params], lr=rate)\n",
    "\n",
    "    training_loop(\n",
    "        n_epochs = 5000, \n",
    "        optimizer = optimizer,\n",
    "        params = params,\n",
    "        train_x = training_x, \n",
    "        val_x = validation_x, \n",
    "        train_y = training_y,\n",
    "        val_y = validation_y\n",
    "    )\n",
    "    \n",
    "    val_predicted = model(validation_x.transpose(-2, 1), *params)\n",
    "    val_loss = loss_function(val_predicted, validation_y)\n",
    "    \n",
    "    print(f\"Loss:\\t\\t{val_loss:.5f}\")\n",
    "    print(f\"Paramaters:\\t{params.detach().numpy()}\")\n",
    "        \n",
    "    print('\\n'+\"--\"*30+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77aa5c7a8032890130e9a412da4828609b1dfa25c62620094c03af7a5cea44b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
