{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0,\n",
    "                    8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
    "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w0, w1, b):\n",
    "    return w0 * t_u**2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(predicted, actual):\n",
    "    squared_diffs = (predicted - actual)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate that the the new model is working\n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does zeroing the optimizer gradient at the begining make a difference \n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad() # <1>\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Significance of T_un\n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Significance of T_un with zero gradient\n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad() # <1>\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.01, 0.001, 0.0001]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_to_learn_at = [1/x for x in [10, 100, 1000, 10000]]\n",
    "rates_to_learn_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c, epoch_report_val = 500):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % epoch_report_val == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding which optimizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 10.708596229553223\n",
      "Epoch 1000, Loss 8.642083168029785\n",
      "Epoch 1500, Loss 7.1710052490234375\n",
      "Epoch 2000, Loss 6.123476982116699\n",
      "Epoch 2500, Loss 5.377227306365967\n",
      "Epoch 3000, Loss 4.845284938812256\n",
      "Epoch 3500, Loss 4.465787887573242\n",
      "Epoch 4000, Loss 4.194724082946777\n",
      "Epoch 4500, Loss 4.0008015632629395\n",
      "Epoch 5000, Loss 3.8617441654205322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5570, -0.8881, -0.8753], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = rates_to_learn_at[3]\n",
    "optimizer = optim.SGD([params], lr=learning_rate) \n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params, \n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 10577728.0\n",
      "Epoch 1000, Loss 9524402.0\n",
      "Epoch 1500, Loss 8545122.0\n",
      "Epoch 2000, Loss 7634292.5\n",
      "Epoch 2500, Loss 6787368.0\n",
      "Epoch 3000, Loss 6000706.0\n",
      "Epoch 3500, Loss 5271407.5\n",
      "Epoch 4000, Loss 4597170.0\n",
      "Epoch 4500, Loss 3976134.25\n",
      "Epoch 5000, Loss 3406753.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5412,  0.5412, -0.4588], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = rates_to_learn_at[3]\n",
    "optimizer = optim.Adam([params], lr=learning_rate) \n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u, \n",
    "    t_c = t_c\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  3,  5,  4, 10,  2,  1,  9,  6]), tensor([8, 7]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices  # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_t_u = t_u[train_indices]\n",
    "training_t_c = t_c[train_indices]\n",
    "\n",
    "validation_t_u = t_u[val_indices]\n",
    "validation_t_c = t_c[val_indices]\n",
    "\n",
    "training_t_un = 0.1 * training_t_u\n",
    "validation_t_un = 0.1 * validation_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "                  train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params) # <1>\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "                             \n",
    "        val_t_p = model(val_t_u, *params) # <1>\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 757.6071, Validation loss 307.6372\n",
      "Epoch 2, Training loss 404.0941, Validation loss 200.9509\n",
      "Epoch 3, Training loss 217.4629, Validation loss 138.7690\n",
      "Epoch 500, Training loss 7.0935, Validation loss 31.3754\n",
      "Epoch 1000, Training loss 5.8806, Validation loss 27.4340\n",
      "Epoch 1500, Training loss 5.0004, Validation loss 24.2863\n",
      "Epoch 2000, Training loss 4.3616, Validation loss 21.7569\n",
      "Epoch 2500, Training loss 3.8980, Validation loss 19.7124\n",
      "Epoch 3000, Training loss 3.5614, Validation loss 18.0503\n",
      "Epoch 3500, Training loss 3.3171, Validation loss 16.6921\n",
      "Epoch 4000, Training loss 3.1396, Validation loss 15.5765\n",
      "Epoch 4500, Training loss 3.0107, Validation loss 14.6561\n",
      "Epoch 5000, Training loss 2.9170, Validation loss 13.8935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5017, -0.5374, -0.6002], requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = training_t_un, # <1> \n",
    "    val_t_u = validation_t_un, # <1> \n",
    "    train_t_c = training_t_c,\n",
    "    val_t_c = validation_t_c)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "                  train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        with torch.no_grad(): # <1>\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False # <2>\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 757.6071, Validation loss 307.6372\n",
      "Epoch 2, Training loss 404.0941, Validation loss 200.9509\n",
      "Epoch 3, Training loss 217.4629, Validation loss 138.7690\n",
      "Epoch 500, Training loss 7.0935, Validation loss 31.3754\n",
      "Epoch 1000, Training loss 5.8806, Validation loss 27.4340\n",
      "Epoch 1500, Training loss 5.0004, Validation loss 24.2863\n",
      "Epoch 2000, Training loss 4.3616, Validation loss 21.7569\n",
      "Epoch 2500, Training loss 3.8980, Validation loss 19.7124\n",
      "Epoch 3000, Training loss 3.5614, Validation loss 18.0503\n",
      "Epoch 3500, Training loss 3.3171, Validation loss 16.6921\n",
      "Epoch 4000, Training loss 3.1396, Validation loss 15.5765\n",
      "Epoch 4500, Training loss 3.0107, Validation loss 14.6561\n",
      "Epoch 5000, Training loss 2.9170, Validation loss 13.8935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5017, -0.5374, -0.6002], requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = training_t_un, # <1> \n",
    "    val_t_u = validation_t_un, # <1> \n",
    "    train_t_c = training_t_c,\n",
    "    val_t_c = validation_t_c)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward(t_u, t_c, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77aa5c7a8032890130e9a412da4828609b1dfa25c62620094c03af7a5cea44b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
