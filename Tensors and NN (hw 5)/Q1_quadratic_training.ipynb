{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0,\n",
    "                    8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
    "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w0, w1, b):\n",
    "    return w0 * t_u**2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(predicted, actual):\n",
    "    squared_diffs = (predicted - actual)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate that the the new model is working\n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does zeroing the optimizer gradient at the begining make a difference \n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad() # <1>\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Significance of T_un\n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3139e+03, -3.5181e+01, -5.9642e-01], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Significance of T_un with zero gradient\n",
    "\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad() # <1>\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.01, 0.001, 0.0001, 1e-05]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_to_learn_at = [1/x for x in [10, 100, 1000, 10000, 100000]]\n",
    "rates_to_learn_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c, epoch_report_val = 500):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % epoch_report_val == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding which optimizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 10.708596229553223\n",
      "Epoch 1000, Loss 8.642083168029785\n",
      "Epoch 1500, Loss 7.1710052490234375\n",
      "Epoch 2000, Loss 6.123478412628174\n",
      "Epoch 2500, Loss 5.377227306365967\n",
      "Epoch 3000, Loss 4.8452863693237305\n",
      "Epoch 3500, Loss 4.465787887573242\n",
      "Epoch 4000, Loss 4.194724082946777\n",
      "Epoch 4500, Loss 4.0008015632629395\n",
      "Epoch 5000, Loss 3.8617441654205322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5570, -0.8881, -0.8753], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = rates_to_learn_at[3]\n",
    "optimizer = optim.SGD([params], lr=learning_rate) \n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params, \n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 10577728.0\n",
      "Epoch 1000, Loss 9524402.0\n",
      "Epoch 1500, Loss 8545122.0\n",
      "Epoch 2000, Loss 7634292.5\n",
      "Epoch 2500, Loss 6787368.0\n",
      "Epoch 3000, Loss 6000706.0\n",
      "Epoch 3500, Loss 5271407.5\n",
      "Epoch 4000, Loss 4597170.0\n",
      "Epoch 4500, Loss 3976134.25\n",
      "Epoch 5000, Loss 3406753.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5412,  0.5412, -0.4588], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = rates_to_learn_at[3]\n",
    "optimizer = optim.Adam([params], lr=learning_rate) \n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u, \n",
    "    t_c = t_c\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model is subjective but loss seems to be better with SGD with higher optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop at different learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.1 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss nan\n",
      "Epoch 1000, Loss nan\n",
      "Epoch 1500, Loss nan\n",
      "Epoch 2000, Loss nan\n",
      "Epoch 2500, Loss nan\n",
      "Epoch 3000, Loss nan\n",
      "Epoch 3500, Loss nan\n",
      "Epoch 4000, Loss nan\n",
      "Epoch 4500, Loss nan\n",
      "Epoch 5000, Loss nan\n",
      "Final Paramaters: tensor([nan, nan, nan], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.01 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss nan\n",
      "Epoch 1000, Loss nan\n",
      "Epoch 1500, Loss nan\n",
      "Epoch 2000, Loss nan\n",
      "Epoch 2500, Loss nan\n",
      "Epoch 3000, Loss nan\n",
      "Epoch 3500, Loss nan\n",
      "Epoch 4000, Loss nan\n",
      "Epoch 4500, Loss nan\n",
      "Epoch 5000, Loss nan\n",
      "Final Paramaters: tensor([nan, nan, nan], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss nan\n",
      "Epoch 1000, Loss nan\n",
      "Epoch 1500, Loss nan\n",
      "Epoch 2000, Loss nan\n",
      "Epoch 2500, Loss nan\n",
      "Epoch 3000, Loss nan\n",
      "Epoch 3500, Loss nan\n",
      "Epoch 4000, Loss nan\n",
      "Epoch 4500, Loss nan\n",
      "Epoch 5000, Loss nan\n",
      "Final Paramaters: tensor([nan, nan, nan], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.0001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 10.708596229553223\n",
      "Epoch 1000, Loss 8.642083168029785\n",
      "Epoch 1500, Loss 7.1710052490234375\n",
      "Epoch 2000, Loss 6.123478412628174\n",
      "Epoch 2500, Loss 5.377227306365967\n",
      "Epoch 3000, Loss 4.8452863693237305\n",
      "Epoch 3500, Loss 4.465787887573242\n",
      "Epoch 4000, Loss 4.194724082946777\n",
      "Epoch 4500, Loss 4.0008015632629395\n",
      "Epoch 5000, Loss 3.8617441654205322\n",
      "Final Paramaters: tensor([ 0.5570, -0.8881, -0.8753], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 1e-05 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 13.269524574279785\n",
      "Epoch 1000, Loss 12.944255828857422\n",
      "Epoch 1500, Loss 12.629863739013672\n",
      "Epoch 2000, Loss 12.325987815856934\n",
      "Epoch 2500, Loss 12.032271385192871\n",
      "Epoch 3000, Loss 11.748376846313477\n",
      "Epoch 3500, Loss 11.473977088928223\n",
      "Epoch 4000, Loss 11.20875358581543\n",
      "Epoch 4500, Loss 10.952396392822266\n",
      "Epoch 5000, Loss 10.704610824584961\n",
      "Final Paramaters: tensor([ 0.3155,  0.5401, -0.1712], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate in rates_to_learn_at:\n",
    "    print(f\"Learning rate is {rate} :\\n\"+\"/\\\\\"*15)\n",
    "    \n",
    "    params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "    learning_rate = rate\n",
    "    optimizer = optim.SGD([params], lr=learning_rate) \n",
    "\n",
    "    training_loop(\n",
    "        n_epochs = 5000, \n",
    "        optimizer = optimizer,\n",
    "        params = params, \n",
    "        t_u = t_un,\n",
    "        t_c = t_c)\n",
    "    \n",
    "    print(f\"Final Paramaters: {params}\")\n",
    "    \n",
    "    print('\\n'+\"--\"*30+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.1 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 2.7825067043304443\n",
      "Epoch 1000, Loss 2.4860265254974365\n",
      "Epoch 1500, Loss 2.2615137100219727\n",
      "Epoch 2000, Loss 2.144075393676758\n",
      "Epoch 2500, Loss 2.101926565170288\n",
      "Epoch 3000, Loss 2.092149019241333\n",
      "Epoch 3500, Loss 2.0908169746398926\n",
      "Epoch 4000, Loss 2.0907232761383057\n",
      "Epoch 4500, Loss 2.090721368789673\n",
      "Epoch 5000, Loss 2.090721368789673\n",
      "Final Paramaters: tensor([  0.2830,   2.4760, -10.6496], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.01 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 6.111172199249268\n",
      "Epoch 1000, Loss 3.936776638031006\n",
      "Epoch 1500, Loss 3.1178040504455566\n",
      "Epoch 2000, Loss 2.931839942932129\n",
      "Epoch 2500, Loss 2.8712592124938965\n",
      "Epoch 3000, Loss 2.8129403591156006\n",
      "Epoch 3500, Loss 2.7440879344940186\n",
      "Epoch 4000, Loss 2.664674997329712\n",
      "Epoch 4500, Loss 2.5763678550720215\n",
      "Epoch 5000, Loss 2.482455253601074\n",
      "Final Paramaters: tensor([ 0.4673,  0.4768, -5.6706], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 103.7950668334961\n",
      "Epoch 1000, Loss 13.01849365234375\n",
      "Epoch 1500, Loss 8.064861297607422\n",
      "Epoch 2000, Loss 7.688989162445068\n",
      "Epoch 2500, Loss 7.295181751251221\n",
      "Epoch 3000, Loss 6.830940246582031\n",
      "Epoch 3500, Loss 6.3061723709106445\n",
      "Epoch 4000, Loss 5.739599227905273\n",
      "Epoch 4500, Loss 5.1592116355896\n",
      "Epoch 5000, Loss 4.600076198577881\n",
      "Final Paramaters: tensor([ 0.4484, -0.0524, -1.7755], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.0001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 578.252685546875\n",
      "Epoch 1000, Loss 491.23663330078125\n",
      "Epoch 1500, Loss 413.86767578125\n",
      "Epoch 2000, Loss 345.2539367675781\n",
      "Epoch 2500, Loss 284.6673889160156\n",
      "Epoch 3000, Loss 231.5107421875\n",
      "Epoch 3500, Loss 185.28346252441406\n",
      "Epoch 4000, Loss 145.55223083496094\n",
      "Epoch 4500, Loss 111.92176055908203\n",
      "Epoch 5000, Loss 84.0093765258789\n",
      "Final Paramaters: tensor([ 0.5721,  0.5698, -0.4337], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 1e-05 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 665.5360717773438\n",
      "Epoch 1000, Loss 655.3656616210938\n",
      "Epoch 1500, Loss 645.3062133789062\n",
      "Epoch 2000, Loss 635.353759765625\n",
      "Epoch 2500, Loss 625.4799194335938\n",
      "Epoch 3000, Loss 615.684814453125\n",
      "Epoch 3500, Loss 606.0101928710938\n",
      "Epoch 4000, Loss 596.4282836914062\n",
      "Epoch 4500, Loss 586.9239501953125\n",
      "Epoch 5000, Loss 577.4971313476562\n",
      "Final Paramaters: tensor([ 0.9506,  0.9505, -0.0495], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate in rates_to_learn_at:\n",
    "    print(f\"Learning rate is {rate} :\\n\"+\"/\\\\\"*15)\n",
    "    \n",
    "    params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "    learning_rate = rate\n",
    "    optimizer = optim.Adam([params], lr=learning_rate) \n",
    "\n",
    "    training_loop(\n",
    "        n_epochs = 5000, \n",
    "        optimizer = optimizer,\n",
    "        params = params, \n",
    "        t_u = t_un,\n",
    "        t_c = t_c)\n",
    "    \n",
    "    print(f\"Final Paramaters: {params}\")\n",
    "    \n",
    "    print('\\n'+\"--\"*30+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.1 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 5.5968017578125\n",
      "Epoch 1000, Loss 4.2388691902160645\n",
      "Epoch 1500, Loss 3.5880417823791504\n",
      "Epoch 2000, Loss 3.2693517208099365\n",
      "Epoch 2500, Loss 3.109431266784668\n",
      "Epoch 3000, Loss 3.026103973388672\n",
      "Epoch 3500, Loss 2.9799187183380127\n",
      "Epoch 4000, Loss 2.951834201812744\n",
      "Epoch 4500, Loss 2.932614803314209\n",
      "Epoch 5000, Loss 2.917778730392456\n",
      "Final Paramaters: tensor([ 0.5440, -0.3944, -3.3654], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.01 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 130.22694396972656\n",
      "Epoch 1000, Loss 51.7548828125\n",
      "Epoch 1500, Loss 24.96746063232422\n",
      "Epoch 2000, Loss 14.81622314453125\n",
      "Epoch 2500, Loss 10.820043563842773\n",
      "Epoch 3000, Loss 9.208422660827637\n",
      "Epoch 3500, Loss 8.536825180053711\n",
      "Epoch 4000, Loss 8.238479614257812\n",
      "Epoch 4500, Loss 8.088947296142578\n",
      "Epoch 5000, Loss 7.999006748199463\n",
      "Final Paramaters: tensor([ 0.3664,  0.3244, -0.7380], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 590.3999633789062\n",
      "Epoch 1000, Loss 556.1788940429688\n",
      "Epoch 1500, Loss 530.8682250976562\n",
      "Epoch 2000, Loss 510.1600341796875\n",
      "Epoch 2500, Loss 492.3868408203125\n",
      "Epoch 3000, Loss 476.6943359375\n",
      "Epoch 3500, Loss 462.5755920410156\n",
      "Epoch 4000, Loss 449.7004089355469\n",
      "Epoch 4500, Loss 437.83905029296875\n",
      "Epoch 5000, Loss 426.8250732421875\n",
      "Final Paramaters: tensor([ 0.8656,  0.8655, -0.1347], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.0001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 666.8950805664062\n",
      "Epoch 1000, Loss 663.1060180664062\n",
      "Epoch 1500, Loss 660.20849609375\n",
      "Epoch 2000, Loss 657.7723388671875\n",
      "Epoch 2500, Loss 655.6311645507812\n",
      "Epoch 3000, Loss 653.6987915039062\n",
      "Epoch 3500, Loss 651.9282836914062\n",
      "Epoch 4000, Loss 650.2799072265625\n",
      "Epoch 4500, Loss 648.7376098632812\n",
      "Epoch 5000, Loss 647.2752075195312\n",
      "Final Paramaters: tensor([ 0.9861,  0.9861, -0.0139], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 1e-05 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 674.8992919921875\n",
      "Epoch 1000, Loss 674.517822265625\n",
      "Epoch 1500, Loss 674.223876953125\n",
      "Epoch 2000, Loss 673.9776611328125\n",
      "Epoch 2500, Loss 673.7567749023438\n",
      "Epoch 3000, Loss 673.5718994140625\n",
      "Epoch 3500, Loss 673.3876342773438\n",
      "Epoch 4000, Loss 673.203369140625\n",
      "Epoch 4500, Loss 673.0196533203125\n",
      "Epoch 5000, Loss 672.8961181640625\n",
      "Final Paramaters: tensor([ 0.9986,  0.9986, -0.0014], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate in rates_to_learn_at:\n",
    "    print(f\"Learning rate is {rate} :\\n\"+\"/\\\\\"*15)\n",
    "    \n",
    "    params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "    learning_rate = rate\n",
    "    optimizer = optim.Adagrad([params], lr=learning_rate) \n",
    "\n",
    "    training_loop(\n",
    "        n_epochs = 5000, \n",
    "        optimizer = optimizer,\n",
    "        params = params, \n",
    "        t_u = t_un,\n",
    "        t_c = t_c)\n",
    "    \n",
    "    print(f\"Final Paramaters: {params}\")\n",
    "    \n",
    "    print('\\n'+\"--\"*30+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.1 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss nan\n",
      "Epoch 1000, Loss nan\n",
      "Epoch 1500, Loss nan\n",
      "Epoch 2000, Loss nan\n",
      "Epoch 2500, Loss nan\n",
      "Epoch 3000, Loss nan\n",
      "Epoch 3500, Loss nan\n",
      "Epoch 4000, Loss nan\n",
      "Epoch 4500, Loss nan\n",
      "Epoch 5000, Loss nan\n",
      "Final Paramaters: tensor([nan, nan, nan], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.01 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss nan\n",
      "Epoch 1000, Loss nan\n",
      "Epoch 1500, Loss nan\n",
      "Epoch 2000, Loss nan\n",
      "Epoch 2500, Loss nan\n",
      "Epoch 3000, Loss nan\n",
      "Epoch 3500, Loss nan\n",
      "Epoch 4000, Loss nan\n",
      "Epoch 4500, Loss nan\n",
      "Epoch 5000, Loss nan\n",
      "Final Paramaters: tensor([nan, nan, nan], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss nan\n",
      "Epoch 1000, Loss nan\n",
      "Epoch 1500, Loss nan\n",
      "Epoch 2000, Loss nan\n",
      "Epoch 2500, Loss nan\n",
      "Epoch 3000, Loss nan\n",
      "Epoch 3500, Loss nan\n",
      "Epoch 4000, Loss nan\n",
      "Epoch 4500, Loss nan\n",
      "Epoch 5000, Loss nan\n",
      "Final Paramaters: tensor([nan, nan, nan], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 0.0001 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 10.708602905273438\n",
      "Epoch 1000, Loss 8.642095565795898\n",
      "Epoch 1500, Loss 7.171025276184082\n",
      "Epoch 2000, Loss 6.123502731323242\n",
      "Epoch 2500, Loss 5.377254486083984\n",
      "Epoch 3000, Loss 4.8453145027160645\n",
      "Epoch 3500, Loss 4.465814113616943\n",
      "Epoch 4000, Loss 4.1947503089904785\n",
      "Epoch 4500, Loss 4.000824451446533\n",
      "Epoch 5000, Loss 3.861766815185547\n",
      "Final Paramaters: tensor([ 0.5570, -0.8881, -0.8753], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Learning rate is 1e-05 :\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Epoch 500, Loss 13.269524574279785\n",
      "Epoch 1000, Loss 12.944255828857422\n",
      "Epoch 1500, Loss 12.629866600036621\n",
      "Epoch 2000, Loss 12.3259859085083\n",
      "Epoch 2500, Loss 12.032269477844238\n",
      "Epoch 3000, Loss 11.748379707336426\n",
      "Epoch 3500, Loss 11.473982810974121\n",
      "Epoch 4000, Loss 11.208756446838379\n",
      "Epoch 4500, Loss 10.952401161193848\n",
      "Epoch 5000, Loss 10.704614639282227\n",
      "Final Paramaters: tensor([ 0.3155,  0.5401, -0.1712], requires_grad=True)\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate in rates_to_learn_at:\n",
    "    print(f\"Learning rate is {rate} :\\n\"+\"/\\\\\"*15)\n",
    "    \n",
    "    params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "    learning_rate = rate\n",
    "    optimizer = optim.ASGD([params], lr=learning_rate) \n",
    "\n",
    "    training_loop(\n",
    "        n_epochs = 5000, \n",
    "        optimizer = optimizer,\n",
    "        params = params, \n",
    "        t_u = t_un,\n",
    "        t_c = t_c)\n",
    "    \n",
    "    print(f\"Final Paramaters: {params}\")\n",
    "    \n",
    "    print('\\n'+\"--\"*30+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77aa5c7a8032890130e9a412da4828609b1dfa25c62620094c03af7a5cea44b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
