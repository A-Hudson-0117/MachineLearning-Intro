{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_map = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}\n",
    "data_path = '../dataset/cifar10'\n",
    "mean = [0.4914, 0.4822, 0.4465] # copied from Q1\n",
    "std = [0.2470, 0.2435, 0.2616] # copied from Q1\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "cifar10_train = data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_test = data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Models For Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_in = 3\n",
    "n_channel_1  = 16\n",
    "n_channel_2  = 32\n",
    "n_out = len(cifar10_map)\n",
    "\n",
    "# Define the structure of the CNN\n",
    "class ConvolutionalModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # img is 32 x 32\n",
    "        self.conv1 = nn.Conv2d(n_channel_in, n_channel_1, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        # img is 16 x 16\n",
    "        self.conv2 = nn.Conv2d(n_channel_1, n_channel_2, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        # img is 8 x 8\n",
    "        \n",
    "        # Add a fully connected layer at the end of the CNN\n",
    "        self.fc_nn = nn.Linear(n_channel_2 * 8 * 8, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolutional filters, activation functions, and pooling layers\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        \n",
    "        # Flatten the output of the convolutional layers\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # Apply the fully connected layer\n",
    "        out = self.fc_nn(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_in = 3\n",
    "n_channel_1  = 24\n",
    "n_out = len(cifar10_map)\n",
    "\n",
    "# Define the structure of the CNN\n",
    "class ConvolutionalModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # img is 32 x 32\n",
    "        self.conv1 = nn.Conv2d(n_channel_in, n_channel_1, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        # img is 16 x 16\n",
    "        \n",
    "        # Add a fully connected layer at the end of the CNN\n",
    "        self.fc = nn.Linear(n_channel_1 * 16 * 16, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolutional filters, activation functions, and pooling layers\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        \n",
    "        # Flatten the output of the convolutional layers\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # Apply the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, training_imgs, loss_fn, optimizer, n_epochs:int, report_period:int = 10):\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        for images, labels in training_imgs:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch % report_period == 0) or (epoch == n_epochs):\n",
    "            print(f'epoch {epoch}:\\t Loss: {loss:.15f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Layer Convolutional Neural Network\n",
    "Question 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalModel1(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=6144, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalModel1()\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 1e-2\n",
    "report_rate = 2\n",
    "n_epochs = 300\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\t Loss: 1.938525557518005\n",
      "epoch 2:\t Loss: 1.555944681167603\n",
      "epoch 4:\t Loss: 1.597625613212585\n",
      "epoch 6:\t Loss: 1.556369662284851\n",
      "epoch 8:\t Loss: 1.517513632774353\n",
      "epoch 10:\t Loss: 1.359889507293701\n",
      "epoch 12:\t Loss: 1.656130552291870\n",
      "epoch 14:\t Loss: 1.983206272125244\n",
      "epoch 16:\t Loss: 1.727459549903870\n",
      "epoch 18:\t Loss: 1.855589866638184\n",
      "epoch 20:\t Loss: 1.453886866569519\n",
      "epoch 22:\t Loss: 1.246202349662781\n",
      "epoch 24:\t Loss: 1.378020048141479\n",
      "epoch 26:\t Loss: 1.940169334411621\n",
      "epoch 28:\t Loss: 1.477449536323547\n",
      "epoch 30:\t Loss: 1.280393123626709\n",
      "epoch 32:\t Loss: 1.690582752227783\n",
      "epoch 34:\t Loss: 1.533900737762451\n",
      "epoch 36:\t Loss: 1.376434683799744\n",
      "epoch 38:\t Loss: 1.585449814796448\n",
      "epoch 40:\t Loss: 1.675060272216797\n",
      "epoch 42:\t Loss: 1.463064074516296\n",
      "epoch 44:\t Loss: 1.794202923774719\n",
      "epoch 46:\t Loss: 2.209526062011719\n",
      "epoch 48:\t Loss: 1.587497711181641\n",
      "epoch 50:\t Loss: 1.452328801155090\n",
      "epoch 52:\t Loss: 1.565370440483093\n",
      "epoch 54:\t Loss: 1.770161747932434\n",
      "epoch 56:\t Loss: 1.924884676933289\n",
      "epoch 58:\t Loss: 1.543199896812439\n",
      "epoch 60:\t Loss: 2.017888784408569\n",
      "epoch 62:\t Loss: 1.548756003379822\n",
      "epoch 64:\t Loss: 1.038384199142456\n",
      "epoch 66:\t Loss: 1.393439292907715\n",
      "epoch 68:\t Loss: 1.601192831993103\n",
      "epoch 70:\t Loss: 1.630953669548035\n",
      "epoch 72:\t Loss: 1.534579277038574\n",
      "epoch 74:\t Loss: 1.585940241813660\n",
      "epoch 76:\t Loss: 1.594747304916382\n",
      "epoch 78:\t Loss: 1.441074728965759\n",
      "epoch 80:\t Loss: 1.800462961196899\n",
      "epoch 82:\t Loss: 1.366925954818726\n",
      "epoch 84:\t Loss: 1.507715940475464\n",
      "epoch 86:\t Loss: 0.925422310829163\n",
      "epoch 88:\t Loss: 1.550792098045349\n",
      "epoch 90:\t Loss: 1.795894742012024\n",
      "epoch 92:\t Loss: 1.757112860679626\n",
      "epoch 94:\t Loss: 1.542254447937012\n",
      "epoch 96:\t Loss: 1.613867998123169\n",
      "epoch 98:\t Loss: 1.678093671798706\n",
      "epoch 100:\t Loss: 1.831702589988708\n",
      "epoch 102:\t Loss: 1.652629494667053\n",
      "epoch 104:\t Loss: 1.711546897888184\n",
      "epoch 106:\t Loss: 1.134480237960815\n",
      "epoch 108:\t Loss: 1.481565117835999\n",
      "epoch 110:\t Loss: 0.842917621135712\n",
      "epoch 112:\t Loss: 1.448252201080322\n",
      "epoch 114:\t Loss: 1.764099121093750\n",
      "epoch 116:\t Loss: 1.428072690963745\n",
      "epoch 118:\t Loss: 1.305155634880066\n",
      "epoch 120:\t Loss: 1.063513994216919\n",
      "epoch 122:\t Loss: 1.305877327919006\n",
      "epoch 124:\t Loss: 1.686539053916931\n",
      "epoch 126:\t Loss: 1.558272719383240\n",
      "epoch 128:\t Loss: 1.191587567329407\n",
      "epoch 130:\t Loss: 1.515540599822998\n",
      "epoch 132:\t Loss: 1.724395751953125\n",
      "epoch 134:\t Loss: 1.521503806114197\n",
      "epoch 136:\t Loss: 1.535153388977051\n",
      "epoch 138:\t Loss: 1.304990530014038\n",
      "epoch 140:\t Loss: 1.114899992942810\n",
      "epoch 142:\t Loss: 1.844414830207825\n",
      "epoch 144:\t Loss: 1.850285530090332\n",
      "epoch 146:\t Loss: 1.975485801696777\n",
      "epoch 148:\t Loss: 1.330734729766846\n",
      "epoch 150:\t Loss: 1.369630336761475\n",
      "epoch 152:\t Loss: 1.319330573081970\n",
      "epoch 154:\t Loss: 1.933041095733643\n",
      "epoch 156:\t Loss: 1.368329524993896\n",
      "epoch 158:\t Loss: 1.588989853858948\n",
      "epoch 160:\t Loss: 1.264238476753235\n",
      "epoch 162:\t Loss: 1.624169945716858\n",
      "epoch 164:\t Loss: 1.625626325607300\n",
      "epoch 166:\t Loss: 1.701198458671570\n",
      "epoch 168:\t Loss: 2.404094696044922\n",
      "epoch 170:\t Loss: 1.479535341262817\n",
      "epoch 172:\t Loss: 1.672375917434692\n",
      "epoch 174:\t Loss: 1.735538125038147\n",
      "epoch 176:\t Loss: 1.284191489219666\n",
      "epoch 178:\t Loss: 1.650108814239502\n",
      "epoch 180:\t Loss: 1.616137862205505\n",
      "epoch 182:\t Loss: 1.627142190933228\n",
      "epoch 184:\t Loss: 1.689603447914124\n",
      "epoch 186:\t Loss: 1.246268033981323\n",
      "epoch 188:\t Loss: 1.535723090171814\n",
      "epoch 190:\t Loss: 1.277954578399658\n",
      "epoch 192:\t Loss: 1.255121946334839\n",
      "epoch 194:\t Loss: 1.610342144966125\n",
      "epoch 196:\t Loss: 1.267523527145386\n",
      "epoch 198:\t Loss: 1.399110555648804\n",
      "epoch 200:\t Loss: 2.239930629730225\n",
      "epoch 202:\t Loss: 1.872760295867920\n",
      "epoch 204:\t Loss: 1.088719844818115\n",
      "epoch 206:\t Loss: 1.738449215888977\n",
      "epoch 208:\t Loss: 1.209090709686279\n",
      "epoch 210:\t Loss: 1.276629328727722\n",
      "epoch 212:\t Loss: 1.724464774131775\n",
      "epoch 214:\t Loss: 1.546728849411011\n",
      "epoch 216:\t Loss: 1.536120176315308\n",
      "epoch 218:\t Loss: 1.934517025947571\n",
      "epoch 220:\t Loss: 1.167268872261047\n",
      "epoch 222:\t Loss: 1.265433549880981\n",
      "epoch 224:\t Loss: 1.476455569267273\n",
      "epoch 226:\t Loss: 1.321266055107117\n",
      "epoch 228:\t Loss: 1.508257389068604\n",
      "epoch 230:\t Loss: 1.768439888954163\n",
      "epoch 232:\t Loss: 1.956259727478027\n",
      "epoch 234:\t Loss: 1.475311994552612\n",
      "epoch 236:\t Loss: 1.459230065345764\n",
      "epoch 238:\t Loss: 1.855480790138245\n",
      "epoch 240:\t Loss: 1.686022520065308\n",
      "epoch 242:\t Loss: 0.935461342334747\n",
      "epoch 244:\t Loss: 1.120266556739807\n",
      "epoch 246:\t Loss: 1.414164781570435\n",
      "epoch 248:\t Loss: 1.349915027618408\n",
      "epoch 250:\t Loss: 1.761454582214355\n",
      "epoch 252:\t Loss: 1.473971247673035\n",
      "epoch 254:\t Loss: 1.239654898643494\n",
      "epoch 256:\t Loss: 1.696338295936584\n",
      "epoch 258:\t Loss: 1.589649677276611\n",
      "epoch 260:\t Loss: 2.152026414871216\n",
      "epoch 262:\t Loss: 1.542393803596497\n",
      "epoch 264:\t Loss: 1.519717574119568\n",
      "epoch 266:\t Loss: 0.993134856224060\n",
      "epoch 268:\t Loss: 1.385796308517456\n",
      "epoch 270:\t Loss: 1.898967742919922\n",
      "epoch 272:\t Loss: 1.469214677810669\n",
      "epoch 274:\t Loss: 1.597189307212830\n",
      "epoch 276:\t Loss: 1.275958299636841\n",
      "epoch 278:\t Loss: 1.369176983833313\n",
      "epoch 280:\t Loss: 0.934739768505096\n",
      "epoch 282:\t Loss: 1.275488138198853\n",
      "epoch 284:\t Loss: 1.280707120895386\n",
      "epoch 286:\t Loss: 1.973171830177307\n",
      "epoch 288:\t Loss: 1.996097683906555\n",
      "epoch 290:\t Loss: 1.587050795555115\n",
      "epoch 292:\t Loss: 1.829909324645996\n",
      "epoch 294:\t Loss: 1.662554502487183\n",
      "epoch 296:\t Loss: 2.133751869201660\n",
      "epoch 298:\t Loss: 2.195323705673218\n"
     ]
    }
   ],
   "source": [
    "training(model, \n",
    "         cifar10_train, \n",
    "         loss, \n",
    "         optimizer, \n",
    "         n_epochs,\n",
    "         report_period=report_rate\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 50000\n",
      "Accuracy: 0.50274\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the CNN on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in cifar10_train:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Report the training time, training loss, and evaluation accuracy\n",
    "print(f'Total: {total}')\n",
    "print(f'Accuracy: {correct / total:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 10000\n",
      "Accuracy: 0.36430\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the CNN on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in cifar10_test:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Report the training time, training loss, and evaluation accuracy\n",
    "print(f'Total: {total}')\n",
    "print(f'Accuracy: {correct / total:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer Convolutional Neural Network\n",
    "Question 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalModel2(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc_nn): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalModel2()\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 1e-2\n",
    "report_rate = 2\n",
    "n_epochs = 300\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\t Loss: 2.312840223312378\n",
      "epoch 2:\t Loss: 2.311159133911133\n",
      "epoch 4:\t Loss: 2.297081708908081\n",
      "epoch 6:\t Loss: 2.320553779602051\n",
      "epoch 8:\t Loss: 2.310359001159668\n",
      "epoch 10:\t Loss: 2.304403781890869\n",
      "epoch 12:\t Loss: 2.308589458465576\n",
      "epoch 14:\t Loss: 2.288920640945435\n",
      "epoch 16:\t Loss: 2.312175035476685\n",
      "epoch 18:\t Loss: 2.311953783035278\n",
      "epoch 20:\t Loss: 2.317118883132935\n",
      "epoch 22:\t Loss: 2.306055307388306\n",
      "epoch 24:\t Loss: 2.312306165695190\n",
      "epoch 26:\t Loss: 2.304031133651733\n",
      "epoch 28:\t Loss: 2.311303377151489\n",
      "epoch 30:\t Loss: 2.294530391693115\n",
      "epoch 32:\t Loss: 2.299096107482910\n",
      "epoch 34:\t Loss: 2.302850723266602\n",
      "epoch 36:\t Loss: 2.312999486923218\n",
      "epoch 38:\t Loss: 2.313287258148193\n",
      "epoch 40:\t Loss: 2.303790330886841\n",
      "epoch 42:\t Loss: 2.301569938659668\n",
      "epoch 44:\t Loss: 2.319022417068481\n",
      "epoch 46:\t Loss: 2.315701007843018\n",
      "epoch 48:\t Loss: 2.288652420043945\n",
      "epoch 50:\t Loss: 2.305642127990723\n",
      "epoch 52:\t Loss: 2.301823854446411\n",
      "epoch 54:\t Loss: 2.303159236907959\n",
      "epoch 56:\t Loss: 2.300716161727905\n",
      "epoch 58:\t Loss: 2.335052728652954\n",
      "epoch 60:\t Loss: 2.293085098266602\n",
      "epoch 62:\t Loss: 2.308187007904053\n",
      "epoch 64:\t Loss: 2.294457197189331\n",
      "epoch 66:\t Loss: 2.316494941711426\n",
      "epoch 68:\t Loss: 2.301618099212646\n",
      "epoch 70:\t Loss: 2.313077926635742\n",
      "epoch 72:\t Loss: 2.305704832077026\n",
      "epoch 74:\t Loss: 2.309107303619385\n",
      "epoch 76:\t Loss: 2.300257444381714\n",
      "epoch 78:\t Loss: 2.322780132293701\n",
      "epoch 80:\t Loss: 2.318464279174805\n",
      "epoch 82:\t Loss: 2.308423519134521\n",
      "epoch 84:\t Loss: 2.307212829589844\n",
      "epoch 86:\t Loss: 2.317685127258301\n",
      "epoch 88:\t Loss: 2.299534559249878\n",
      "epoch 90:\t Loss: 2.313173770904541\n",
      "epoch 92:\t Loss: 2.315126895904541\n",
      "epoch 94:\t Loss: 2.291617870330811\n",
      "epoch 96:\t Loss: 2.298484086990356\n",
      "epoch 98:\t Loss: 2.308118820190430\n",
      "epoch 100:\t Loss: 2.312909126281738\n",
      "epoch 102:\t Loss: 2.309323310852051\n",
      "epoch 104:\t Loss: 2.297343254089355\n",
      "epoch 106:\t Loss: 2.313049554824829\n",
      "epoch 108:\t Loss: 2.316626310348511\n",
      "epoch 110:\t Loss: 2.305021524429321\n",
      "epoch 112:\t Loss: 2.302476644515991\n",
      "epoch 114:\t Loss: 2.303381919860840\n",
      "epoch 116:\t Loss: 2.298395633697510\n",
      "epoch 118:\t Loss: 2.323060035705566\n",
      "epoch 120:\t Loss: 2.304510831832886\n",
      "epoch 122:\t Loss: 2.287716150283813\n",
      "epoch 124:\t Loss: 2.308836936950684\n",
      "epoch 126:\t Loss: 2.297055244445801\n",
      "epoch 128:\t Loss: 2.308661460876465\n",
      "epoch 130:\t Loss: 2.322377443313599\n",
      "epoch 132:\t Loss: 2.302905082702637\n",
      "epoch 134:\t Loss: 2.293885707855225\n",
      "epoch 136:\t Loss: 2.317342758178711\n",
      "epoch 138:\t Loss: 2.313970565795898\n",
      "epoch 140:\t Loss: 2.308836221694946\n",
      "epoch 142:\t Loss: 2.307185649871826\n",
      "epoch 144:\t Loss: 2.302061080932617\n",
      "epoch 146:\t Loss: 2.303214073181152\n",
      "epoch 148:\t Loss: 2.298275709152222\n",
      "epoch 150:\t Loss: 2.322689771652222\n",
      "epoch 152:\t Loss: 2.308049678802490\n",
      "epoch 154:\t Loss: 2.305161476135254\n",
      "epoch 156:\t Loss: 2.330652713775635\n",
      "epoch 158:\t Loss: 2.315556287765503\n",
      "epoch 160:\t Loss: 2.311899185180664\n",
      "epoch 162:\t Loss: 2.292906522750854\n",
      "epoch 164:\t Loss: 2.296946763992310\n",
      "epoch 166:\t Loss: 2.309050559997559\n",
      "epoch 168:\t Loss: 2.296346664428711\n",
      "epoch 170:\t Loss: 2.289997100830078\n",
      "epoch 172:\t Loss: 2.305204391479492\n",
      "epoch 174:\t Loss: 2.310410261154175\n",
      "epoch 176:\t Loss: 2.302946567535400\n",
      "epoch 178:\t Loss: 2.302721500396729\n",
      "epoch 180:\t Loss: 2.307323455810547\n",
      "epoch 182:\t Loss: 2.293334245681763\n",
      "epoch 184:\t Loss: 2.301697731018066\n",
      "epoch 186:\t Loss: 2.279818534851074\n",
      "epoch 188:\t Loss: 2.302234411239624\n",
      "epoch 190:\t Loss: 2.301189899444580\n",
      "epoch 192:\t Loss: 2.313195705413818\n",
      "epoch 194:\t Loss: 2.312571525573730\n",
      "epoch 196:\t Loss: 2.293109178543091\n",
      "epoch 198:\t Loss: 2.310602903366089\n",
      "epoch 200:\t Loss: 2.308619737625122\n",
      "epoch 202:\t Loss: 2.301826238632202\n",
      "epoch 204:\t Loss: 2.312649965286255\n",
      "epoch 206:\t Loss: 2.301790952682495\n",
      "epoch 208:\t Loss: 2.313593387603760\n",
      "epoch 210:\t Loss: 2.314896821975708\n",
      "epoch 212:\t Loss: 2.303961277008057\n",
      "epoch 214:\t Loss: 2.309731483459473\n",
      "epoch 216:\t Loss: 2.305100202560425\n",
      "epoch 218:\t Loss: 2.310792922973633\n",
      "epoch 220:\t Loss: 2.298523426055908\n",
      "epoch 222:\t Loss: 2.286164045333862\n",
      "epoch 224:\t Loss: 2.297972202301025\n",
      "epoch 226:\t Loss: 2.294048786163330\n",
      "epoch 228:\t Loss: 2.306660413742065\n",
      "epoch 230:\t Loss: 2.328718662261963\n",
      "epoch 232:\t Loss: 2.285796880722046\n",
      "epoch 234:\t Loss: 2.306283235549927\n",
      "epoch 236:\t Loss: 2.288663387298584\n",
      "epoch 238:\t Loss: 2.309642553329468\n",
      "epoch 240:\t Loss: 2.312165737152100\n",
      "epoch 242:\t Loss: 2.310715198516846\n",
      "epoch 244:\t Loss: 2.321779489517212\n",
      "epoch 246:\t Loss: 2.304254531860352\n",
      "epoch 248:\t Loss: 2.297779083251953\n",
      "epoch 250:\t Loss: 2.304816246032715\n",
      "epoch 252:\t Loss: 2.313294410705566\n",
      "epoch 254:\t Loss: 2.321994543075562\n",
      "epoch 256:\t Loss: 2.322771072387695\n",
      "epoch 258:\t Loss: 2.318087100982666\n",
      "epoch 260:\t Loss: 2.294507980346680\n",
      "epoch 262:\t Loss: 2.306652784347534\n",
      "epoch 264:\t Loss: 2.299922943115234\n",
      "epoch 266:\t Loss: 2.305518865585327\n",
      "epoch 268:\t Loss: 2.312891006469727\n",
      "epoch 270:\t Loss: 2.318443298339844\n",
      "epoch 272:\t Loss: 2.311308622360229\n",
      "epoch 274:\t Loss: 2.308231115341187\n",
      "epoch 276:\t Loss: 2.288707017898560\n",
      "epoch 278:\t Loss: 2.351792812347412\n",
      "epoch 280:\t Loss: 2.311571121215820\n",
      "epoch 282:\t Loss: 2.303634643554688\n",
      "epoch 284:\t Loss: 2.324807405471802\n",
      "epoch 286:\t Loss: 2.305658817291260\n",
      "epoch 288:\t Loss: 2.308709144592285\n",
      "epoch 290:\t Loss: 2.317495822906494\n",
      "epoch 292:\t Loss: 2.303014278411865\n",
      "epoch 294:\t Loss: 2.284006834030151\n",
      "epoch 296:\t Loss: 2.298321008682251\n",
      "epoch 298:\t Loss: 2.330623865127563\n"
     ]
    }
   ],
   "source": [
    "training(model, \n",
    "         cifar10_train, \n",
    "         loss, \n",
    "         optimizer, \n",
    "         n_epochs,\n",
    "         report_period=report_rate\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 50000\n",
      "Accuracy: 0.10000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the CNN on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in cifar10_train:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Report the training time, training loss, and evaluation accuracy\n",
    "print(f'Total: {total}')\n",
    "print(f'Accuracy: {correct / total:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 10000\n",
      "Accuracy: 0.10000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the CNN on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in cifar10_test:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Report the training time, training loss, and evaluation accuracy\n",
    "print(f'Total: {total}')\n",
    "print(f'Accuracy: {correct / total:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77aa5c7a8032890130e9a412da4828609b1dfa25c62620094c03af7a5cea44b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
